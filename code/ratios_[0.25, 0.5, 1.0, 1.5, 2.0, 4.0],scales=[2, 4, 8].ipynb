{"cells":[{"cell_type":"markdown","metadata":{"id":"EeCohpL5LNAH"},"source":["# Baseline Model\n"," - Oriented_rcnn_r50_fpn_1x_dota_le90"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtQIsLscLLzG"},"outputs":[],"source":["# Check nvcc version\n","!nvcc -V\n","# Check GCC version\n","!gcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmpnzQ_QLu23"},"outputs":[],"source":["!pip install wandb -qU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMA6D5GYLv7x"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9_Ys93rLwz-"},"outputs":[],"source":["!pip install openmim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgeb2E1VLx4-"},"outputs":[],"source":["!mim install mmcv-full===1.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMb239GTLzkO"},"outputs":[],"source":["!mim install mmdet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sv0Lim2Qxlvd"},"outputs":[],"source":["!mim install mmengine"]},{"cell_type":"markdown","metadata":{"id":"R6gEzD-jL1U_"},"source":["# Git clone part\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aCALWIoL03f"},"outputs":[],"source":["!git clone https://github.com/open-mmlab/mmrotate.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-WtVQnaL4PI"},"outputs":[],"source":["%cd mmrotate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okCQ-lnIL5Bh"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VB73NC53L53A"},"outputs":[],"source":["!pip install -r requirements/build.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JEfxdtiL7MG"},"outputs":[],"source":["!pip install -e ."]},{"cell_type":"markdown","metadata":{"id":"NstJxSVnL-8_"},"source":["# Oriented_rcnn download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05wrFJ15L-AY"},"outputs":[],"source":["!mim download mmrotate --config oriented_rcnn_r50_fpn_1x_dota_le90 --dest ."]},{"cell_type":"markdown","metadata":{"id":"fg6gN-RtMGiQ"},"source":["#Google Drive mount"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"earyV5JXMGC2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXLIFVBCMj_-"},"outputs":[],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3y0bP28MLdm"},"outputs":[],"source":["import os\n","from zipfile import ZipFile\n","import time\n","\n","# unzip dataset \n","def extract_zip(zip_path,output_path):\n","  \n","  \"\"\"\n","  Extract .zip File\n","  Args\n","          - zip_path(str) : The path of zip file  \n","  \"\"\"\n","  os.makedirs(output_path, exist_ok=True)\n","  \n","  with ZipFile(zip_path) as zip:\n","      start_time = time.time()\n","      zip.extractall(output_path) \n","      print\n","      print(zip_path,'Unzip Complete')\n","      print(zip_path,\" Unzip Time: {:.4f}sec\".format((time.time() - start_time)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmXkaDIXNP-v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664240970579,"user_tz":-540,"elapsed":583799,"user":{"displayName":"윤혜연","userId":"13048533309979940862"}},"outputId":"0be8037d-7412-42e1-e9f1-6e0aad57cf1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/00.AIFFEL/03.해커톤/00.SIA 해커톤/00.잘할거SIA/data/train_images(sub_all_type).zip Unzip Complete\n","/content/drive/MyDrive/00.AIFFEL/03.해커톤/00.SIA 해커톤/00.잘할거SIA/data/train_images(sub_all_type).zip  Unzip Time: 577.5717sec\n","/content/drive/MyDrive/00.AIFFEL/03.해커톤/00.SIA 해커톤/00.잘할거SIA/data/train_labelTxt_main.zip Unzip Complete\n","/content/drive/MyDrive/00.AIFFEL/03.해커톤/00.SIA 해커톤/00.잘할거SIA/data/train_labelTxt_main.zip  Unzip Time: 1.3349sec\n"]}],"source":["# train\n","dataset_path = '/content/drive/MyDrive/00.AIFFEL/03.해커톤/00.SIA 해커톤/00.잘할거SIA/data/'\n","\n","train_img_path = os.path.join(dataset_path,'train_images(sub_all_type).zip')\n","train_ann_path = os.path.join(dataset_path,'train_labelTxt_main.zip')\n","\n","output_path = '/content/mmrotate/data/split_ss_dota/'\n","os.makedirs(output_path, exist_ok=True)\n","\n","\n","train_output_path = '/content/mmrotate/data/split_ss_dota/train'\n","\n","\n","# unzip \n","extract_zip(train_img_path,'/content/mmrotate/data/split_ss_dota/train/images')\n","extract_zip(train_ann_path,'/content/mmrotate/data/split_ss_dota/train/annfiles')\n","\n","# os.rename('/content/mmrotate/data/split_ss_dota/train/train_annfiles','/content/mmrotate/data/split_ss_dota/train/annfiles')\n","# os.rename('/content/mmrotate/data/split_ss_dota/train/train_images','/content/mmrotate/data/split_ss_dota/train/images')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1lc4tPtMPI-"},"outputs":[],"source":["# validation\n","dataset_path = '/content/drive/MyDrive/00.AIFFEL/03.해커톤/00.SIA 해커톤/00.잘할거SIA/data/'\n","\n","validation_img_path = os.path.join(dataset_path,'val_images(sub_all_type).zip')\n","validation_ann_path = os.path.join(dataset_path,'val_labelTxt_main.zip')\n","\n","output_path = '/content/mmrotate/data/split_ss_dota/'\n","os.makedirs(output_path, exist_ok=True)\n","\n","\n","val_output_path = '/content/mmrotate/data/split_ss_dota/val'\n","\n","\n","# unzip \n","extract_zip(validation_img_path,val_output_path + '/images')\n","extract_zip(validation_ann_path,val_output_path + '/annfiles')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWrh1TjkOoR6"},"outputs":[],"source":["# convert tif to png  (-> )\n","\n","def convert_tif_to_png(img_dir):\n","    import os\n","    tif_files = os.listdir(img_dir)\n","    for tif_file in tif_files:\n","        png_file = tif_file.replace('.tif','.png')\n","        os.replace(f\"{img_dir}/{tif_file}\", f\"{img_dir}/{png_file}\")\n","    print(img_dir,': convert complete')\n","\n","train_img_dir = '/content/mmrotate/data/split_ss_dota/train/images'\n","val_img_dir = '/content/mmrotate/data/split_ss_dota/val/images'\n","\n","convert_tif_to_png(train_img_dir)\n","convert_tif_to_png(val_img_dir)"]},{"cell_type":"markdown","metadata":{"id":"qUtlAkNPMpuf"},"source":["#Config file "]},{"cell_type":"markdown","metadata":{"id":"a_QUqhsHFL-r"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mc0ObJSNG-n"},"outputs":[],"source":["\n","########################## 새로운 데이터셋 정의 : FAIR1MDataset 클래스를 생성하여 fair1m.py 생성하기 ############################\n","\n","FAIR1MDataset_text = '''\n","\n","from .builder import ROTATED_DATASETS\n","from .dota import DOTADataset\n","import os\n","import os.path as osp\n","import xml.etree.ElementTree as ET\n","from collections import OrderedDict\n","import numpy as np\n","from PIL import Image\n","from mmrotate.core import eval_rbbox_map\n","\n","\n","@ROTATED_DATASETS.register_module()\n","class FAIR1MDataset(DOTADataset):\n","    \"\"\"fair1m dataset for detection \"\"\"\n","    CLASSES = ('Airplane','Ship', 'Vehicle')\n","    PALETTE = [(165, 42, 42), (189, 183, 107), (0, 255, 0)]\n","\n","    def __init__(self, **kwargs):\n","        super(FAIR1MDataset, self).__init__(**kwargs)\n","\n","    def evaluate(self,\n","                 results,\n","                 metric= ['mAP','F1Score'],\n","                 logger=None,\n","                 proposal_nums=(100, 300, 1000),\n","                 iou_thr=0.5,\n","                 scale_ranges=None,\n","                 nproc=4):\n","\n","        \"\"\"Evaluate the dataset.\n","        Args:\n","            results (list): Testing results of the dataset.\n","            metric (str | list[str]): Metrics to be evaluated.\n","            logger (logging.Logger | None | str): Logger used for printing\n","                related information during evaluation. Default: None.\n","            proposal_nums (Sequence[int]): Proposal number used for evaluating\n","                recalls, such as recall@100, recall@1000.\n","                Default: (100, 300, 1000).\n","            iou_thr (float | list[float]): IoU threshold. It must be a float\n","                when evaluating mAP, and can be a list when evaluating recall.\n","                Default: 0.5.\n","            scale_ranges (list[tuple] | None): Scale ranges for evaluating mAP.\n","                Default: None.\n","            use_07_metric (bool): Whether to use the voc07 metric.\n","            nproc (int): Processes used for computing TP and FP.\n","                Default: 4.\n","        \"\"\"\n","        \n","        # super().evaluate()\n","        nproc = min(nproc, os.cpu_count())\n","\n","        # metric 이 없으면 mAP,F1Score 쓰기\n","        if not isinstance(metric, str):\n","            assert len(metric) == 2\n","            metric = metric\n","        \n","        # allowed_metrics 외 metric을 적으면 해당 메트릭은 지원하지 않는다는 예외처리하기   \n","        allowed_metrics = ['mAP','F1Score']\n","        if metric not in allowed_metrics:\n","            raise KeyError(f'metric {metric} is not supported')\n","\n","        annotations = [self.get_ann_info(i) for i in range(len(self))]\n","        eval_results = {}\n","\n","\n","        # mAP, F1Score metric 정의 (metric으로 mAP, F1Score 두개 다 썼을때 )\n","        if ('mAP' and 'F1Score') in metric  :\n","            assert isinstance(iou_thr, float)\n","\n","            mean_ap, mean_f1, _ = eval_rbbox_map(\n","                results,\n","                annotations,\n","                scale_ranges=scale_ranges,\n","                iou_thr=iou_thr,\n","                dataset=self.CLASSES,\n","                logger=logger,\n","                nproc=nproc)\n","            eval_results['mAP'] = mean_ap\n","            eval_results['F1Score'] = mean_f1\n","\n","        # mAP metric 정의 (mAP 일때는 iou별 AP 도 같이 계산해야함)\n","        if metric == 'mAP' :\n","            assert isinstance(iou_thr, float)\n","            mean_ap, _, _ = eval_rbbox_map(\n","                results,\n","                annotations,\n","                scale_ranges=scale_ranges,\n","                iou_thr=iou_thr,\n","                dataset=self.CLASSES,\n","                logger=logger,\n","                nproc=nproc)\n","            eval_results['mAP'] = mean_ap\n","\n","\n","        # F1Score metric 정의    \n","        elif metric == 'F1Score':\n","            assert isinstance(iou_thr, float)\n","            _, mean_f1, _ = eval_rbbox_map(\n","                results,\n","                annotations,\n","                scale_ranges=scale_ranges,\n","                iou_thr=iou_thr,\n","                dataset=self.CLASSES,\n","                logger=logger,\n","                nproc=nproc)\n","            eval_results['F1Score'] = mean_f1\n","\n","\n","        else:\n","            raise NotImplementedError\n","\n","        return eval_results\n","        \n","'''\n","\n","FAIR1MDataset_path = '/content/mmrotate/mmrotate/datasets/fair1m.py'\n","\n","f = open(FAIR1MDataset_path, \"w\")\n","f.write(FAIR1MDataset_text)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wScteQSFEfdH"},"outputs":[],"source":["FAIR1MDataset_init = '''\n","# Copyright (c) OpenMMLab. All rights reserved.\n","from .builder import build_dataset  # noqa: F401, F403\n","from .dota import DOTADataset  # noqa: F401, F403\n","from .hrsc import HRSCDataset  # noqa: F401, F403\n","from .pipelines import *  # noqa: F401, F403\n","from .sar import SARDataset  # noqa: F401, F403\n","from .fair1m import FAIR1MDataset\n","\n","__all__ = ['SARDataset', 'FAIR1MDataset', 'DOTADataset', 'build_dataset', 'HRSCDataset']\n","'''\n","\n","FAIR1MDataset_init_path = '/content/mmrotate/mmrotate/datasets/__init__.py'\n","\n","f = open(FAIR1MDataset_init_path, \"w\")\n","f.write(FAIR1MDataset_init)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"XF0GCL_YFOdk"},"source":["## eval_map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4lC_5XbFRk5"},"outputs":[],"source":["########## eval_map ############\n","\n","eval_map = '''\n","\n","# Copyright (c) OpenMMLab. All rights reserved.\n","from multiprocessing import get_context\n","\n","import numpy as np\n","import torch\n","from mmcv.ops import box_iou_rotated\n","from mmcv.utils import print_log\n","from mmdet.core import average_precision\n","from terminaltables import AsciiTable\n","\n","# tpfp 가려내는 함수 -> 밑에선 클래스 별로 계산함\n","def tpfp_default(det_bboxes,\n","                 gt_bboxes,\n","                 gt_bboxes_ignore=None,\n","                 iou_thr=0.5,\n","                 area_ranges=None):\n","    \"\"\"Check if detected bboxes are true positive or false positive.\n","\n","    Args:\n","        det_bboxes (ndarray): Detected bboxes of this image, of shape (m, 6).\n","        gt_bboxes (ndarray): GT bboxes of this image, of shape (n, 5).\n","        gt_bboxes_ignore (ndarray): Ignored gt bboxes of this image,\n","            of shape (k, 5). Default: None\n","        iou_thr (float): IoU threshold to be considered as matched.\n","            Default: 0.5.\n","        area_ranges (list[tuple] | None): Range of bbox areas to be evaluated,\n","            in the format [(min1, max1), (min2, max2), ...]. Default: None.\n","\n","    Returns:\n","        tuple[np.ndarray]: (tp, fp) whose elements are 0 and 1. The shape of\n","            each array is (num_scales, m).\n","    \"\"\"\n","\n","    # an indicator of ignored gts\n","    det_bboxes = np.array(det_bboxes)\n","    gt_ignore_inds = np.concatenate(\n","        # gt_bboxes는 밑에서 결과에 따라 0,1로 바꾸는거고. gt_bboxes_ignore은 우선 ignore 이니까 다 1로 해놓음\n","        (np.zeros(gt_bboxes.shape[0], dtype=np.bool),\n","         np.ones(gt_bboxes_ignore.shape[0], dtype=np.bool)))\n","    # stack gt_bboxes and gt_bboxes_ignore for convenience\n","    gt_bboxes = np.vstack((gt_bboxes, gt_bboxes_ignore))\n","\n","    num_dets = det_bboxes.shape[0]\n","    num_gts = gt_bboxes.shape[0]\n","    if area_ranges is None:\n","        area_ranges = [(None, None)]\n","    num_scales = len(area_ranges) # num_scales == 1 is None else 2???\n","    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp of\n","    # a certain scale [[ 0, 이 num_dets개 ]]\n","    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n","    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n","\n","####################### 이미지에 gt가 없음 ########################################\n","    # 이번 이미지에서 gt가 없으면 det전부를 fp로 변경\n","    if gt_bboxes.shape[0] == 0:\n","        if area_ranges == [(None, None)]:\n","            # fp 전체를 1로 바꿈\n","            fp[...] = 1\n","        else:\n","            raise NotImplementedError\n","        return tp, fp\n","\n","######################## 이미지에 gt가 있음 #########################################\n","    # 이미지에 gt가 있는 상태에서 iou에따른 tp, fp 계산\n","    ious = box_iou_rotated(\n","        torch.from_numpy(det_bboxes).float(),\n","        torch.from_numpy(gt_bboxes).float()).numpy()\n","\n","    # for each det, the max iou with all gts\n","    # 열 중에서(axis = 1) 큰 원소를 반환\n","    ious_max = ious.max(axis=1)\n","    # for each det, which gt overlaps most with it\n","    # 열 중에서(axis = 1) 큰 원소의 인덱스를 반환\n","    ious_argmax = ious.argmax(axis=1)\n","    # sort all dets in descending order by scores 스코어 제일 높은 값 sort_inds??\n","    sort_inds = np.argsort(-det_bboxes[:, -1]) # shape of det_bboxes : (m, 6)\n","    for k, (min_area, max_area) in enumerate(area_ranges):\n","        gt_covered = np.zeros(num_gts, dtype=bool)\n","        # area_ranges가 지정되지 않은 경우 gt_area_ignore는 모두 False입니다.\n","        if min_area is None:\n","            gt_area_ignore = np.zeros_like(gt_ignore_inds, dtype=bool)\n","        else:\n","            raise NotImplementedError\n","        for i in sort_inds:\n","            # 스코어 높은 값을 하나씩 꺼내서 그 값이 IOU thres 보다 높으면 그걸로 gt랑 매칭함\n","            if ious_max[i] >= iou_thr:\n","                matched_gt = ious_argmax[i]\n","                if not (gt_ignore_inds[matched_gt]\n","                        or gt_area_ignore[matched_gt]):\n","                    if not gt_covered[matched_gt]:\n","                        gt_covered[matched_gt] = True\n","                    # 매칭한 gt가 ignore이 아니고 gts에 있으면 tp / 아니면 fp\n","                        tp[k, i] = 1\n","                    else:\n","                        fp[k, i] = 1\n","                # otherwise ignore this detected bbox, tp = 0, fp = 0\n","            elif min_area is None:\n","                fp[k, i] = 1\n","            else:\n","                bbox = det_bboxes[i, :5]\n","                area = bbox[2] * bbox[3]\n","                if area >= min_area and area < max_area:\n","                    fp[k, i] = 1\n","    return tp, fp\n","\n","\n","# 디텍팅한 결과값을 가져와서 우리가 원하는 클래스에 있으면, 클래스 별로 \n","def get_cls_results(det_results, annotations, class_id):\n","    \"\"\"Get det results and gt information of a certain class.\n","\n","    Args:\n","        det_results (list[list]): \n","            det_results (list[list]): [[cls1_det, cls2_det, ...], ...].\n","            The outer list indicates images, \n","            and the inner list indicates per-class detected bboxes.\n","            => len(det_results) == 이미지 갯수, len(det_results[0]) == 클래스별로 감지된 bbox 갯수 \n","        annotations (list[dict]): Same as `eval_map()`.\n","        class_id (int): ID of a specific class.\n","\n","    Returns:\n","        tuple[list[np.ndarray]]: detected bboxes, gt bboxes, ignored gt bboxes\n","    \"\"\"\n","    # 디텍트 결과에서 이미지별 클래스 뽑아냄\n","    cls_dets = [img_res[class_id] for img_res in det_results]\n","\n","    cls_gts = []\n","    cls_gts_ignore = []\n","    for ann in annotations:\n","        #어노테이션의 라벨이 클래스 id에 있으면 그 박스들을 cls_gt에 넣음\n","        gt_inds = ann['labels'] == class_id\n","        cls_gts.append(ann['bboxes'][gt_inds, :])\n","        # labels_ignore 값이 있으면 그 클래스 id는 cls_gts_ignore 값에 넣음\n","        if ann.get('labels_ignore', None) is not None:\n","            ignore_inds = ann['labels_ignore'] == class_id\n","            cls_gts_ignore.append(ann['bboxes_ignore'][ignore_inds, :])\n","\n","        else:\n","            cls_gts_ignore.append(torch.zeros((0, 5), dtype=torch.float64))\n","\n","    #cls_dets : 총 디텍트 결과, cls_gts : 디텍팅 결과에서 지정 클래스인것, cls_gts_ignore : 무시 값 \n","    return cls_dets, cls_gts, cls_gts_ignore\n","\n","\n","def eval_rbbox_map(det_results,\n","                   annotations,\n","                   scale_ranges=None,\n","                   iou_thr=0.5,\n","                   use_07_metric=True,\n","                   dataset=None,\n","                   logger=None,\n","                   nproc=4):\n","    \"\"\"Evaluate mAP of a rotated dataset.\n","\n","    Args:\n","        det_results (list[list]): 이미지 마다 디텍팅한 클래스 별 bbox\n","            [[cls1_det, cls2_det, ...], ...].\n","            The outer list indicates images, and the inner list indicates\n","            per-class detected bboxes.\n","        annotations (list[dict]): gt annotation an image\n","        Ground truth annotations where each item of the list indicates an image. \n","        Keys of annotations are:\n","            - `bboxes`: numpy array of shape (n, 5) => n개가 5포인트로\n","            - `labels`: numpy array of shape (n, ) => n개의 클래스\n","            - `bboxes_ignore` (optional): numpy array of shape (k, 5) => 무시할 비박스\n","            - `labels_ignore` (optional): numpy array of shape (k, ) => 무시할 클래스\n","\n","        scale_ranges (list[tuple] | None): 계산할 size min-max 지정\n","            Range of scales to be evaluated, in the format [(min1, max1), (min2, max2), ...]. \n","            A range of (32, 64) means the area range between (32**2, 64**2).\n","            Default: None.\n","\n","        iou_thr (float): IoU threshold to be considered as matched.\n","            Default: 0.5.\n","\n","        use_07_metric (bool): Whether to use the voc07 metric.\n","        dataset (list[str] | str | None): Dataset name or dataset classes,\n","            there are minor differences in metrics for different datasets, e.g.\n","            \"voc07\", \"imagenet_det\", etc. Default: None.\n","\n","        logger (logging.Logger | str | None): The way to print the mAP\n","            summary. See `mmcv.utils.print_log()` for details. Default: None.\n","        nproc (int): Processes used for computing TP and FP.\n","            Default: 4.\n","\n","    Returns:\n","        tuple: (mAP, [dict, dict, ...])\n","    \"\"\"\n","    #이미지수랑 어노테이션 수 안맞으면 오류!\n","    assert len(det_results) == len(annotations)\n","\n","\n","    num_imgs = len(det_results)\n","    num_scales = len(scale_ranges) if scale_ranges is not None else 1\n","    num_classes = len(det_results[0])  # positive class num // len(det_results[0]) == 클래스별로 감지된 bbox 갯수 \n","    area_ranges = ([(rg[0]**2, rg[1]**2) for rg in scale_ranges]\n","                   if scale_ranges is not None else None)\n","\n","    pool = get_context('spawn').Pool(nproc)\n","    eval_results = []\n","    # 클래스 별로 계산\n","    for i in range(num_classes):\n","        # get gt and det bboxes of this class\n","        #cls_dets : 총 디텍트 결과, cls_gts : 디텍팅 결과에서 지정 클래스인것, cls_gts_ignore : 무시 값 \n","        cls_dets, cls_gts, cls_gts_ignore = get_cls_results(\n","            det_results, annotations, i)\n","\n","        # compute tp and fp for each image with multiple processes\n","        tpfp = pool.starmap(\n","            tpfp_default,\n","            zip(cls_dets, cls_gts, cls_gts_ignore,\n","                [iou_thr for _ in range(num_imgs)],\n","                [area_ranges for _ in range(num_imgs)]))\n","        tp, fp = tuple(zip(*tpfp))\n","\n","        # calculate gt number of each scale\n","        # ignored gts or gts beyond the specific scale are not counted\n","        # num_gts : area_ranges 없으면 디텍팅 결과에서 지정 클래스인 수, 있으면 사이즈에 맞는 수만 들어감\n","        num_gts = np.zeros(num_scales, dtype=int)\n","        for _, bbox in enumerate(cls_gts):\n","            if area_ranges is None:\n","                num_gts[0] += bbox.shape[0]\n","            else:\n","                gt_areas = bbox[:, 2] * bbox[:, 3]\n","                for k, (min_area, max_area) in enumerate(area_ranges):\n","                    num_gts[k] += np.sum((gt_areas >= min_area)\n","                                         & (gt_areas < max_area))\n","        # sort all det bboxes by score, also sort tp and fp\n","        cls_dets = np.vstack(cls_dets)\n","        num_dets = cls_dets.shape[0]\n","        sort_inds = np.argsort(-cls_dets[:, -1])\n","        tp = np.hstack(tp)[:, sort_inds]\n","        fp = np.hstack(fp)[:, sort_inds]\n","\n","        # calculate recall and precision with tp and fp\n","        tp = np.cumsum(tp, axis=1)\n","        fp = np.cumsum(fp, axis=1)\n","        eps = np.finfo(np.float32).eps\n","        recalls = tp / np.maximum(num_gts[:, np.newaxis], eps) # num_gts가 gt수니까\n","        precisions = tp / np.maximum((tp + fp), eps)\n","\n","        # calculate AP\n","        # scale_ranges 지정 되어 있으면 rec, prec, num_gts 값 변경\n","        if scale_ranges is None:\n","            recalls = recalls[0, :]\n","            precisions = precisions[0, :]\n","            num_gts = num_gts.item()\n","        mode = 'area' if not use_07_metric else '11points'\n","        ap = average_precision(recalls, precisions, mode)\n","        # calculate F1 추가\n","        f1_score = 2 * precisions * recalls / np.maximum((precisions+recalls),eps)\n","        f1_score = f1_score.mean()\n","\n","        eval_results.append({\n","            'num_gts': num_gts,\n","            'num_dets': num_dets,\n","            'recall': recalls,\n","            'precision': precisions,\n","            'ap': ap,\n","            'F1' : f1_score\n","        })\n","    pool.close()\n","\n","    # 클래스 별로 계산끝@ eval_results에 클래스별로 값 들어가있음\n","    \n","    # 만약 scale_ranges 가 지정되어 있으면,\n","    if scale_ranges is not None:\n","        # shape (num_classes, num_scales)\n","        ##### mF1 score 추가\n","        all_ap = np.vstack([cls_result['ap'] for cls_result in eval_results])\n","        all_f1 = np.vstack([cls_result['F1'] for cls_result in eval_results])\n","        all_num_gts = np.vstack(\n","            [cls_result['num_gts'] for cls_result in eval_results])\n","        mean_ap = []\n","        mean_f1 = []\n","        # 스케일별로 mean값 뽑아서 mean_리스트에 넣음 [1클래스 ap 평균, 2클래스 ap 평균, ...]\n","        for i in range(num_scales):\n","            if np.any(all_num_gts[:, i] > 0):\n","                mean_ap.append(all_ap[all_num_gts[:, i] > 0, i].mean())\n","                mean_f1.append(all_f1[all_num_gts[:, i] > 0, i].mean())\n","            else:\n","                mean_ap.append(0.0)\n","                mean_f1.append(0.0)\n","\n","    # scale_ranges 없으면 클래스별로 리스트에 넣어서 평균냄\n","    else:\n","        aps = []\n","        f1s = []\n","\n","        for cls_result in eval_results:\n","            # 클래스의 gt가 있으면, 각 클래스별 ap, f1값 각각 리스트에 넣음 aps = [1클래스 ap, 2클래스 ap, ...]\n","            if cls_result['num_gts'] > 0:\n","                aps.append(cls_result['ap'])\n","                f1s.append(cls_result['F1'])\n","\n","        mean_ap = np.array(aps).mean() if aps else 0.0\n","        mean_f1 = np.array(f1s).mean() if f1s else 0.0\n","\n","    print_map_summary(\n","        mean_ap, mean_f1, eval_results, dataset, area_ranges, logger=logger)\n","\n","    return mean_ap, mean_f1, eval_results\n","    \n","    # eval_results에 클래스별로 걊들어가있음\n","\n","def print_map_summary(mean_ap,\n","                      mean_f1,\n","                      results,\n","                      dataset=None,\n","                      scale_ranges=None,\n","                      logger=None):\n","\n","    \"\"\"Print mAP and results of each class.\n","\n","    A table will be printed to show the gts/dets/recall/AP/F1!!!!! of each class and\n","    the mAP,mF1.\n","\n","    Args:\n","        mean_ap (float): Calculated from `eval_map()`.\n","        mean_f1 (list): Calculated from `eval_map()`. \n","            mean_f1 = [weighted_f1, macro_f1]\n","        results (list[dict]): Calculated from `eval_map()`.\n","        dataset (list[str] | str | None): Dataset name or dataset classes.\n","        scale_ranges (list[tuple] | None): Range of scales to be evaluated.\n","        logger (logging.Logger | str | None): The way to print the mAP\n","            summary. See `mmcv.utils.print_log()` for details. Default: None.\n","    \"\"\"\n","\n","    if logger == 'silent':\n","        return\n","\n","    # scale_ranges 있었는지 확인하는 듯\n","    if isinstance(results[0]['ap'], np.ndarray):\n","        num_scales = len(results[0]['ap']) # 클래스 1의 ap?\n","    else:\n","        num_scales = 1\n","\n","    # scale_ranges가 있으면, 계산할때랑 맞는지 확인함\n","    if scale_ranges is not None:\n","        assert len(scale_ranges) == num_scales\n","\n","    num_classes = len(results)\n","\n","    # 각 값들 (0,0,0)으로 세팅\n","    recalls = np.zeros((num_scales, num_classes), dtype=np.float32) \n","    precisions = np.zeros((num_scales, num_classes), dtype=np.float32) \n","    aps = np.zeros((num_scales, num_classes), dtype=np.float32)\n","    f1s = np.zeros((num_scales, num_classes), dtype=np.float32)\n","    num_gts = np.zeros((num_scales, num_classes), dtype=int)\n","    for i, cls_result in enumerate(results):\n","        #클래스별로 꺼내는데, recall값이 있으면,\n","        if cls_result['recall'].size > 0:\n","            recalls[:, i] = np.array(cls_result['recall'], ndmin=2)[:, -1]\n","            precisions[:, i] = np.array(cls_result['precision'], ndmin=2)[:, -1]\n","        aps[:, i] = cls_result['ap']\n","        f1s[:, i] = cls_result['F1']\n","        num_gts[:, i] = cls_result['num_gts']\n","\n","    if dataset is None:\n","        label_names = [str(i) for i in range(num_classes)]\n","    else:\n","        label_names = dataset\n","\n","    if not isinstance(mean_ap, list):\n","        mean_ap = [mean_ap]\n","\n","    if not isinstance(mean_f1, list):\n","        mean_f1 = [mean_f1]\n","\n","    header = ['class', 'gts', 'dets', 'recall', 'precision', 'ap', 'F1']\n","    for i in range(num_scales):\n","        if scale_ranges is not None:\n","            print_log(f'Scale range {scale_ranges[i]}', logger=logger)\n","        table_data = [header]\n","        for j in range(num_classes):\n","            row_data = [\n","                label_names[j], num_gts[i, j], results[j]['num_dets'],\n","                f'{recalls[i, j]:.3f}', f'{precisions[i, j]:.3f}', f'{aps[i, j]:.3f}', f'{f1s[i, j]:.3f}'\n","            ]\n","            table_data.append(row_data)\n","        append_data = ['mAP, mF1', '', '', '','', f'{mean_ap[i]:.3f}', f'{mean_f1[i]:.3f}']\n","        table_data.append(append_data)\n","        table = AsciiTable(table_data)\n","        table.inner_footing_row_border = True\n","        print_log('\\\\n' + table.table, logger=logger)\n","\n","'''\n","\n","eval_map_path = '/content/mmrotate/mmrotate/core/evaluation/eval_map.py'\n","\n","f = open(eval_map_path, \"w\")\n","f.write(eval_map)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"TGZmC4xkFviT"},"source":["## model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFeXRXe-Fw17"},"outputs":[],"source":["model_config = '''\n","\n","dataset_type = 'FAIR1MDataset'\n","data_root = '/content/mmrotate/data/split_ss_dota/'\n","img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations', with_bbox=True),\n","    dict(type='RResize', img_scale=(1024, 1024), multiscale_mode='range'),###################### <---------------- multiscale_mode='range' 추가완료 #####################\n","    dict(type='RRandomCrop', crop_size=(512, 512)), ###################### <---------------- RRandomCrop추가완료 #####################\n","    dict(\n","        type='RRandomFlip',\n","        flip_ratio=[0.25, 0.25, 0.25],\n","        direction=['horizontal', 'vertical', 'diagonal'],\n","        version='le90'),\n","    dict(\n","        type='Normalize',\n","        mean=[123.675, 116.28, 103.53],\n","        std=[58.395, 57.12, 57.375],\n","        to_rgb=True),\n","    dict(type='Pad', size_divisor=32),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(1024, 1024),\n","        flip=False,\n","        transforms=[\n","            dict(type='RResize'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size_divisor=32),\n","            dict(type='DefaultFormatBundle'),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]\n","data = dict(\n","    samples_per_gpu=2,\n","    workers_per_gpu=2,\n","    train=dict(\n","        type='FAIR1MDataset',\n","        ann_file='/content/mmrotate/data/split_ss_dota/train/annfiles/',\n","        img_prefix='/content/mmrotate/data/split_ss_dota/train/images/',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(type='LoadAnnotations', with_bbox=True),\n","            dict(type='RResize', img_scale=(1024, 1024), multiscale_mode='range'),\n","            dict(type='RRandomCrop', crop_size=(512, 512)),\n","            dict(\n","                type='RRandomFlip',\n","                flip_ratio=[0.25, 0.25, 0.25],\n","                direction=['horizontal', 'vertical', 'diagonal'],\n","                version='le90'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size_divisor=32),\n","            dict(type='DefaultFormatBundle'),\n","            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n","        ],\n","        version='le90'),\n","    val=dict(\n","        type='FAIR1MDataset',\n","        ann_file='/content/mmrotate/data/split_ss_dota/val/annfiles/',\n","        img_prefix='/content/mmrotate/data/split_ss_dota/val/images/',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1024, 1024),\n","                flip=False,\n","                transforms=[\n","                    dict(type='RResize'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='Pad', size_divisor=32),\n","                    dict(type='DefaultFormatBundle'),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ],\n","        version='le90'),\n","    test=dict(\n","        type='FAIR1MDataset',\n","        ann_file='/content/mmrotate/data/split_ss_dota/val/annfiles/',\n","        img_prefix='/content/mmrotate/data/split_ss_dota/val/images/',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1024, 1024),\n","                flip=False,\n","                transforms=[\n","                    dict(type='RResize'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='Pad', size_divisor=32),\n","                    dict(type='DefaultFormatBundle'),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ],\n","        version='le90'))\n","evaluation = dict(interval=1, metric='mAP' and 'F1Score')\n","optimizer = dict(type='SGD', lr=0.005, momentum=0.9, weight_decay=0.0001)\n","optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n","lr_config = dict(\n","    policy='step',\n","    warmup='linear',\n","    warmup_iters=500,\n","    warmup_ratio=0.3333333333333333,\n","    step=[8, 11])\n","runner = dict(type='EpochBasedRunner', max_epochs=12)\n","checkpoint_config = dict(interval=1)\n","log_config = dict(\n","    interval=50,\n","    hooks=[\n","        dict(type='TextLoggerHook', interval=50),\n","        dict(type='WandbLoggerHook',interval=100,\n","            init_kwargs=dict(\n","                project='SIA_Project',\n","                entity = 'jhgsia',\n","                name = 'ratios_[0.25, 0.5, 1.0, 1.5, 2.0, 4.0],scales=[2, 4, 8]' ###################### <---------------- 여기서 프로젝트에 들어가는 이름 설정#####################\n","            ),\n","            )\n","    ])\n","dist_params = dict(backend='nccl')\n","log_level = 'INFO'\n","load_from = None\n","resume_from = None\n","workflow = [('train', 1)]\n","opencv_num_threads = 0\n","mp_start_method = 'fork'\n","angle_version = 'le90'\n","model = dict(\n","    type='OrientedRCNN',\n","    backbone=dict(\n","        type='ResNet',\n","        depth=50,\n","        num_stages=4,\n","        out_indices=(0, 1, 2, 3),\n","        frozen_stages=1,\n","        norm_cfg=dict(type='BN', requires_grad=True),\n","        norm_eval=True,\n","        style='pytorch',\n","        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n","    neck=dict(\n","        type='FPN',\n","        in_channels=[256, 512, 1024, 2048],\n","        out_channels=256,\n","        num_outs=5),\n","    rpn_head=dict(\n","        type='OrientedRPNHead',\n","        in_channels=256,\n","        feat_channels=256,\n","        version='le90',\n","        anchor_generator=dict(\n","            type='AnchorGenerator',\n","            scales=[2, 4, 8],\n","            ratios=[0.25, 0.5, 1.0, 1.5, 2.0, 4.0],     ###################### <------------여기 자기가 맡은 ratio로 수정 ##################\n","            strides=[4, 8, 16, 32, 64]),\n","        bbox_coder=dict(\n","            type='MidpointOffsetCoder',\n","            angle_range='le90',\n","            target_means=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n","            target_stds=[1.0, 1.0, 1.0, 1.0, 0.5, 0.5]),\n","        loss_cls=dict(\n","            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n","        loss_bbox=dict(\n","            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n","    roi_head=dict(\n","        type='OrientedStandardRoIHead',\n","        bbox_roi_extractor=dict(\n","            type='RotatedSingleRoIExtractor',\n","            roi_layer=dict(\n","                type='RoIAlignRotated',\n","                out_size=7,\n","                sample_num=2,\n","                clockwise=True),\n","            out_channels=256,\n","            featmap_strides=[4, 8, 16, 32]),\n","        bbox_head=dict(\n","            type='RotatedShared2FCBBoxHead',\n","            in_channels=256,\n","            fc_out_channels=1024,\n","            roi_feat_size=7,\n","            num_classes=3,\n","            bbox_coder=dict(\n","                type='DeltaXYWHAOBBoxCoder',\n","                angle_range='le90',\n","                norm_factor=None,\n","                edge_swap=True,\n","                proj_xy=True,\n","                target_means=(0.0, 0.0, 0.0, 0.0, 0.0),\n","                target_stds=(0.1, 0.1, 0.2, 0.2, 0.1)),\n","            reg_class_agnostic=True,\n","            loss_cls=dict(\n","                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n","            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))),\n","    train_cfg=dict(\n","        rpn=dict(\n","            assigner=dict(\n","                type='MaxIoUAssigner',\n","                pos_iou_thr=0.7,\n","                neg_iou_thr=0.3,\n","                min_pos_iou=0.3,\n","                match_low_quality=True,\n","                ignore_iof_thr=-1),\n","            sampler=dict(\n","                type='RandomSampler',\n","                num=256,\n","                pos_fraction=0.5,\n","                neg_pos_ub=-1,\n","                add_gt_as_proposals=False),\n","            allowed_border=0,\n","            pos_weight=-1,\n","            debug=False),\n","        rpn_proposal=dict(\n","            nms_pre=2000,\n","            max_per_img=2000,\n","            nms=dict(type='nms', iou_threshold=0.8),\n","            min_bbox_size=0),\n","        rcnn=dict(\n","            assigner=dict(\n","                type='MaxIoUAssigner',\n","                pos_iou_thr=0.5,\n","                neg_iou_thr=0.5,\n","                min_pos_iou=0.5,\n","                match_low_quality=False,\n","                iou_calculator=dict(type='RBboxOverlaps2D'),\n","                ignore_iof_thr=-1),\n","            sampler=dict(\n","                type='RRandomSampler',\n","                num=512,\n","                pos_fraction=0.25,\n","                neg_pos_ub=-1,\n","                add_gt_as_proposals=True),\n","            pos_weight=-1,\n","            debug=False)),\n","    test_cfg=dict(\n","        rpn=dict(\n","            nms_pre=2000,\n","            max_per_img=2000,\n","            nms=dict(type='nms', iou_threshold=0.8),\n","            min_bbox_size=0),\n","        rcnn=dict(\n","            nms_pre=2000,\n","            min_bbox_size=0,\n","            score_thr=0.05,\n","            nms=dict(iou_thr=0.1),\n","            max_per_img=2000)))\n","\n","'''\n","\n","model_config_path = '/content/mmrotate/configs/oriented_rcnn/oriented_rcnn_r50_fpn_1x_dota_le90.py'\n","\n","f = open(model_config_path, \"w\")\n","f.write(model_config)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"ZI4UyqdXNFRf"},"source":["#Train!"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1664342725088,"user":{"displayName":"윤혜연","userId":"13048533309979940862"},"user_tz":-540},"id":"6qyTifpQMdue","outputId":"10a0ba44-65c0-43c9-f51c-edf62a40422f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/mmrotate'\n","/content\n"]}],"source":["%cd /content/mmrotate"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1664342725090,"user":{"displayName":"윤혜연","userId":"13048533309979940862"},"user_tz":-540},"id":"hT9H0_AoMpfO","outputId":"6cce2f10-5012-4469-ddff-cca4c1d6477f"},"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file './tools/train.py': [Errno 2] No such file or directory\n"]}],"source":["!python ./tools/train.py ./configs/oriented_rcnn/oriented_rcnn_r50_fpn_1x_dota_le90.py"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"12jS8lHu9ztyF1kpiViFGx-scGeD4xFlK","timestamp":1664175046248}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}